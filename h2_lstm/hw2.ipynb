{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0. Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you're looking at this notebook in Colab! \n",
    "1. First, make a copy of this notebook to your local drive, so you can edit it. \n",
    "2. Go ahead and upload the OnionOrNot.csv file from the [assignment zip](https://www.cc.gatech.edu/classes/AY2022/cs4650_fall/programming/h2_torch.zip) in the files panel on the left.\n",
    "3. Right click in the files panel, and select 'Create New Folder' - call this folder src\n",
    "4. Upload all the files in the src/ folder from the [assignment zip](https://www.cc.gatech.edu/classes/AY2022/cs4650_fall/programming/h2_torch.zip) to the src/ folder on colab.\n",
    "\n",
    "***NOTE: REMEMBER TO REGULARLY REDOWNLOAD ALL THE FILES IN SRC FROM COLAB.*** \n",
    "\n",
    "***IF YOU EDIT THE FILES IN COLAB, AND YOU DO NOT REDOWNLOAD THEM, YOU WILL LOSE YOUR WORK!***\n",
    "\n",
    "If you want GPU's, you can always change your instance type to GPU directly in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Loading and Preprocessing Data [10 points]\n",
    "The following cell loads the OnionOrNot dataset, and tokenizes each data item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY #\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import sklearn\n",
    "# this is how we select a GPU if it's avalible on your computer.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.preprocess import clean_text \n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv('train.csv', quotechar='\"')\n",
    "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))\n",
    "\n",
    "# to convert authors into numbers\n",
    "author_to_number = {\n",
    "    'EAP': 0,\n",
    "    'HPL': 1,\n",
    "    'MWS': 2\n",
    "    \n",
    "}\n",
    "\n",
    "# lowercase, removing punctuation and tookenize sentences. Converting labels to int\n",
    "for i in range(len(df)):\n",
    "    df['author'][i] = author_to_number[df['author'][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, process, ,, however, ,, afforded, me, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>1</td>\n",
       "      <td>[it, never, once, occurred, to, me, that, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[in, his, left, hand, was, a, gold, snuff, box...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "      <td>[how, lovely, is, spring, as, we, looked, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>1</td>\n",
       "      <td>[finding, nothing, else, ,, not, even, gold, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...      0   \n",
       "1  id17569  It never once occurred to me that the fumbling...      1   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...      0   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...      2   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...      1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [this, process, ,, however, ,, afforded, me, n...  \n",
       "1  [it, never, once, occurred, to, me, that, the,...  \n",
       "2  [in, his, left, hand, was, a, gold, snuff, box...  \n",
       "3  [how, lovely, is, spring, as, we, looked, from...  \n",
       "4  [finding, nothing, else, ,, not, even, gold, ,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                     id27080\n",
       "text         It was all mud an' water, an' the sky was dark...\n",
       "author                                                       1\n",
       "tokenized    [it, was, all, mud, an, ', water, ,, an, ', th...\n",
       "Name: 42, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded this dataset, we need to split the data into train, validation, and test sets. We also need to create a vocab map for words in our Onion dataset, which will map tokens to numbers. This will be useful later, since torch models can only use tensors of sequences of numbers as inputs. **Go to src/dataset.py, and fill out split_train_val_test, generate_vocab_map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: complete these methods in src/dataset.py\n",
    "from src.dataset import split_train_val_test, generate_vocab_map\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "train_df, val_df, test_df = split_train_val_test(df, props=[.8, .1, .1])\n",
    "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999897849736963, 0.09995403238163338, 0.09995403238163338)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this line of code will help test your implementation\n",
    "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has custom Datset Classes that have very useful extentions. **Go to src/dataset.py, and fill out the HeadlineDataset class.** Refer to PyTorch documentation on Dataset Classes for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import HeadlineDataset\n",
    "from torch.utils.data import RandomSampler\n",
    "#print(train_df)\n",
    "\n",
    "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
    "val_dataset = HeadlineDataset(train_vocab, val_df)\n",
    "test_dataset = HeadlineDataset(train_vocab, test_df)\n",
    "\n",
    "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler = RandomSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use PyTorch DataLoaders to batch our data for us. **Go to src/dataset.py, and fill out collate_fn.** Refer to PyTorch documentation on Dataloaders for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import collate_fn\n",
    "BATCH_SIZE = 16\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  286,    20,    15,    71,  5519,  1764,     8,   463,     2,  6330,\n",
      "          6331,    30,  1280,    26,     2,   375,    26,    22,     1,     5,\n",
      "          3021,  2354,    18,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    1,    57,   187,    10,   275,  2115,     2,   490,  4531,   451,\n",
      "           431,    10,     5,     2,  2692,   196,  5806,  2116,   119,   864,\n",
      "             2,   103,   986,   121,  1350,     1,   109,    47,   150,  2224,\n",
      "           100,  3445,     5,   304,     1,     1,    18,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ 1820,    20,    30,   250,  1502,    80,     2,    27,    37,     1,\n",
      "          6376,    10,   228,   995,   182,    26,     1,     5,  2066,   119,\n",
      "            30,    29,   121,  2698,   112,  1525,    28,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    1,   141,     1,  5992,   196,    10,   393,   295,    98,     7,\n",
      "            10,   855,     5,     1,    18,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  100,    10,    71,   931,  5099,     5,     1,  8175,    10,    20,\n",
      "           187,  1196,  3098,    71,  1468,  4129,    26,    13,   112,  6629,\n",
      "            18,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ 2603,    96,   246,     5,    20,  4911,   109,    29,     1,    18,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [   20,   120,   687,   847,   963,   122,  1523,   963,  2523,     1,\n",
      "            10,   158,    10,    51,   557,    10,    32,    20,   447,   687,\n",
      "           235,  2736,  6205,   217, 10388,   112,   659,    10,   100,    20,\n",
      "             1,   217, 10388,    37,  2310,  6205,  3227,   974,    20,    86,\n",
      "            18,     0,     0,     0],\n",
      "        [  557,    20,   151,   448,    20,  1666,   976,   119,    33,   942,\n",
      "           151,    20,  2232,   112,  2423,  2408,    16,     2,  4271,  1555,\n",
      "             5,  6500,    48,   332,    18,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    5,   109,    20,  2405,     2,     1,    41,  1010,     8,     2,\n",
      "          1542,     8,   102,   559,    10,    20,  1316,    37,   172,  4026,\n",
      "           393,  7547,   428,  1094,     2,     1,    30,     1,    10,   109,\n",
      "           234,   112,  1730,    30,  1668,    37,  1573,   331,   112,   434,\n",
      "            30,    31,   124,    18],\n",
      "        [  403,  4335,   133,   187,   693,    24,   356,     5,     1,    10,\n",
      "           286,    26,  7098,     1,    10,  1654,    10,     1,    24,     2,\n",
      "          6119,    10,  1309,     1,  2515,     2,  2154,    41,   423,    44,\n",
      "          6519,    18,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ 2071,     5,  4988,   196,   338,   131,   669,   134,  6189,    18,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  243,     1,  2014, 10748,    41,    10,    20,   367,   237,    18,\n",
      "           246,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [   20,  1632,   184,   187,   375,    10,   314,     8,  8276,     2,\n",
      "          6020,     8,    37,   722,   127,    18,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [   26,     2,   981,     8, 10258,   130,    20,   151,     1,    37,\n",
      "           831,    28,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ 8535,   182,  2630,   119,     2,  2210,     1,   495,    18,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  340,  2529,   331,   126,   119,     5,   126,  1410,   254,    44,\n",
      "          2845,    46,   334,   847,   468,    44,     1,    13,   328,  2868,\n",
      "            18,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]) tensor([1., 0., 2., 0., 0., 0., 0., 2., 1., 2., 1., 0., 2., 0., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# # Use this to test your collate_fn implementation.\n",
    "\n",
    "# # You can look at the shapes of x and y or put print \n",
    "# # statements in collate_fn while running this snippet\n",
    "\n",
    "for x, y in test_iterator:\n",
    "    print(x,y)\n",
    "    break\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Modeling [10 pts]\n",
    "Let's move to modeling, now that we have dataset iterators that batch our data for us. **Go to src/model.py, and follow the instructions in the file to create a basic neural network. Then, create your model using the class, and define hyperparameters.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ClassificationModel\n",
    "model = None\n",
    "### YOUR CODE GOES HERE (1 line of code) ###\n",
    "model = ClassificationModel(len(train_vocab),embedding_dim=128,hidden_dim = 128,num_layers = 2,bidirectional = True)\n",
    "\n",
    "# model.to(device)\n",
    "# # \n",
    "### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n",
    "\n",
    "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "criterion, optimizer = None, None\n",
    "### YOUR CODE GOES HERE ###\n",
    "criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=0.01)# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Training and Evaluation [10 Points]\n",
    "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the total loss calculated from criterion\n",
    "def train_loop(model, criterion, iterator):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        y = y.long()\n",
    "        ### YOUR CODE STARTS HERE (~6 lines of code) ###\n",
    "        prediction = model(x)\n",
    "        prediction = torch.squeeze(prediction)\n",
    "\n",
    " \n",
    "        loss = criterion(prediction,y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    return total_loss\n",
    "\n",
    "# returns:\n",
    "# - true: a Python boolean array of all the ground truth values \n",
    "#         taken from the dataset iterator\n",
    "# - pred: a Python boolean array of all model predictions. \n",
    "def val_loop(model, criterion, iterator):\n",
    "    true, pred = [], []\n",
    "    ### YOUR CODE STARTS HERE (~8 lines of code) ###\n",
    "    for x, y in tqdm(iterator):\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        # print(\"x\",x)\n",
    "        # print(\"y\",y)  \n",
    "    \n",
    "        preds = model(x)\n",
    "        preds = torch.squeeze(preds)\n",
    "        for i_batch in range(len(y)):\n",
    "            true.append(y[i_batch])\n",
    "            pred.append(torch.argmax(preds[i_batch]))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return true, pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need evaluation metrics that tell us how well our model is doing on the validation set at each epoch. **Complete the functions in src/eval.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:06<00:00, 18.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30270534873618415\n",
      "0.32038834951456313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
    "# It should do pretty poorly.\n",
    "from src.eval_utils import binary_macro_f1, accuracy\n",
    "true, pred = val_loop(model, criterion, val_iterator)\n",
    "true = [x.item() for x in true]\n",
    "pred = [x.item() for x in pred]\n",
    "\n",
    "print(f1_score(true, pred, average='weighted'))\n",
    "print(accuracy_score(true, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Actually training the model [1 point]\n",
    "Watch your model train :D You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [04:22<00:00,  3.73it/s]\n",
      "100%|██████████| 123/123 [00:08<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "TRAIN LOSS: 796.6538567692041\n",
      "VAL F-1: 0.7393125080222425\n",
      "VAL ACC: 0.7404190086867655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [04:25<00:00,  3.68it/s]\n",
      "100%|██████████| 123/123 [00:08<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "TRAIN LOSS: 483.7765866070986\n",
      "VAL F-1: 0.7574967100677433\n",
      "VAL ACC: 0.7577925396014308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [04:08<00:00,  3.94it/s]\n",
      "100%|██████████| 123/123 [00:06<00:00, 17.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2\n",
      "TRAIN LOSS: 390.7471934258938\n",
      "VAL F-1: 0.7612522167700645\n",
      "VAL ACC: 0.7634133878385284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [04:14<00:00,  3.85it/s]\n",
      "100%|██████████| 123/123 [00:07<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3\n",
      "TRAIN LOSS: 327.9839417822659\n",
      "VAL F-1: 0.7667573455520934\n",
      "VAL ACC: 0.7664793050587634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [04:49<00:00,  3.38it/s]\n",
      "100%|██████████| 123/123 [00:08<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4\n",
      "TRAIN LOSS: 328.2313201073557\n",
      "VAL F-1: 0.7706913714309876\n",
      "VAL ACC: 0.7705671946857435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [04:39<00:00,  3.50it/s]\n",
      "100%|██████████| 123/123 [00:10<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5\n",
      "TRAIN LOSS: 319.33898543333635\n",
      "VAL F-1: 0.7576656332052576\n",
      "VAL ACC: 0.7577925396014308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [06:02<00:00,  2.70it/s]\n",
      "100%|██████████| 123/123 [00:08<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6\n",
      "TRAIN LOSS: 292.9661737512797\n",
      "VAL F-1: 0.7584246277150211\n",
      "VAL ACC: 0.7588145120081757\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 7\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    train_loss = train_loop(model, criterion, train_iterator)\n",
    "    true, pred = val_loop(model, criterion, val_iterator)\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    print(f\"TRAIN LOSS: {train_loss}\")\n",
    "    print(f\"VAL F-1: {f1_score(true, pred, average='weighted')}\")\n",
    "    print(f\"VAL ACC: {accuracy_score(true, pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:09<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST F-1: 0.7600362476236181\n",
      "TEST ACC: 0.7603474706182933\n"
     ]
    }
   ],
   "source": [
    "true, pred = val_loop(model, criterion, test_iterator)\n",
    "print(f\"TEST F-1: {f1_score(true, pred, average='weighted')}\")\n",
    "print(f\"TEST ACC: {accuracy_score(true, pred)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd4653ffb3619e38e0f162702933cb5a2e71428b78fc95dca1bdeccba0429964"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
