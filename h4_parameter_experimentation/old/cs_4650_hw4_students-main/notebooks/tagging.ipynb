{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88649f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from models.lstm_network import LSTMNet, train_loop, val_loop\n",
    "from data.data_utils import get_vocabulary, get_unique_labels, get_words_and_labels, split_dataset,\\\n",
    "    word_to_index, word_to_gensim, label_to_index\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for the problem: You'll change these throughout the assignment and see how they affect performance!\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 32\n",
    "NUM_EPOCHS = 15\n",
    "PRE_TRAINED = False\n",
    "BIDIRECTIONAL = True\n",
    "SHUFFLE=False\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_words_and_labels(download=True)  # You can set download=False after your first run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0e920",
   "metadata": {},
   "source": [
    "### Looking at our data\n",
    "Once we have a dataset, we want to know more about it so that we can build a good model. The first questions we probably need to ask are: How many unique words and unique labels exist in the dataset? Assuming we want to model every single word, we need a vocabulary/embedding-matrix that's big enough. Similarly, assuming we want to model every class/label, we need an output layer that's big enough!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to data/data_utils.py\n",
    "# Get the unique word and label lists to know your vocab size\n",
    "unique_words = get_vocabulary(dataset)\n",
    "unique_labels = get_unique_labels(dataset)\n",
    "\n",
    "\n",
    "vocab_size = len(unique_words)\n",
    "output_size = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Num Classes: {output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcf851",
   "metadata": {},
   "source": [
    "### Creating our network:\n",
    "By now, you've created a few LSTMs! Inside `models/lstm_network.py`, you'll find a familiar LSTM class that you need to fill out. Define an embedding matrix, a recurrent layer, a linear output layer, and optionally add activation functions or additional linear layers. For 1c) you will return to the LSTM definition and change your forward pass to take in a set of pre-computed embeddings, instead of taking in a set of indices to your own embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87712d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your LSTM network\n",
    "pred_model = LSTMNet(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embed_dim,\n",
    "        use_embeds=PRE_TRAINED,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_size,\n",
    "        n_layers=num_layers,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "    ).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ede1aa",
   "metadata": {},
   "source": [
    "### Moving from strings to indices:\n",
    "We can't pass strings into our network, we need to pass indices of the embedding matrix. Inside `data/data_utils.py`, you'll find that you need to complete the `word_to_index` function. This should return a dictionary mapping from word to index, so that you can tokenize new strings. For 1c) you will use the gensim word embeddings instead of the embedding matrix from your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ecaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a word mapping\n",
    "if PRE_TRAINED:\n",
    "    word_to_ind = word_to_gensim()\n",
    "else:\n",
    "    word_to_ind = word_to_index(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046efce",
   "metadata": {},
   "source": [
    "### Same thing for labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5489c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a label mapping\n",
    "label_to_ind = label_to_index(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f90be75",
   "metadata": {},
   "source": [
    "### Splitting the dataset:\n",
    "We are going to split this dataset 80/20, training and validation data. Complete the `split_dataset` function in` data/data_utils.py` Using the split percentage (0.2 in this case), figure out where you should split the data and create a training/dev split. For 1b) you will come back to this function and shuffle the data before splitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your training / dev splits\n",
    "train_samples, train_labels, test_samples, test_labels = split_dataset(dataset, word_to_ind, label_to_ind,\n",
    "                                                                           percent_testing=0.2, shuffle=SHUFFLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer and initialize your loss function\n",
    "### YOUR CODE HERE:\n",
    "opt = # Optimizer of your choice\n",
    "criterion = # Loss function of your choice, but cross entropy or negative log likelihood are probably the best bets!\n",
    "### END YOUR CODE\n",
    "training_metrics = {'loss': [], 'acc': []}\n",
    "testing_metrics = {'loss': [], 'acc': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2768dc",
   "metadata": {},
   "source": [
    "### Training and testing:\n",
    "We're almost ready! Now, you have your dataset, your model, and your optimizer. Fill out the remaining functions in `model/lstm_network.py` to calculate the loss for a sample, update the network, and report loss/accuracy scores. The training loop and validation loops should be pretty familiar by now! Remember _do not train on the validation data!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6314f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each epoch, take a training / validation pass over the data. Save training / validation accuracies and losses\n",
    "for _ in range(NUM_EPOCHS):\n",
    "    ### YOUR CODE GOES HERE (determine what to pass to train_loop)\n",
    "    pred_model, loss_total, acc_total = train_loop()\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    training_metrics['loss'].append(loss_total)\n",
    "    training_metrics['acc'].append(acc_total)\n",
    "    print(f\"Training loss: {loss_total} || Training Accuracy: {acc_total}\")\n",
    "    ### YOUR CODE GOES HERE (determine what to pass to val_loop)\n",
    "    loss_total, acc_total = val_loop()\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    testing_metrics['loss'].append(loss_total)\n",
    "    testing_metrics['acc'].append(acc_total)\n",
    "    print(f\"Validation loss: {loss_total} || Validation Accuracy: {acc_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44bcfa7",
   "metadata": {},
   "source": [
    "### Reporting:\n",
    "Now that we've trained a model and gathered some training and testing statistics, let's see how it does! Run the cells below to draw loss and accuracy plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_metrics['loss'], c='red', label='Training Loss')\n",
    "plt.plot(testing_metrics['loss'], c='blue', label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(training_metrics['acc'], c='red', label='Training Accuracy')\n",
    "plt.plot(testing_metrics['acc'], c='blue', label='Testing Accuracy')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}